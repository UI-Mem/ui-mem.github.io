<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents">
  <meta name="keywords" content="UI-Mem, GUI Agents, Reinforcement Learning, LLM, Mobile AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      font-size: 16px;
      line-height: 1.6;
      color: #333;
      background-color: #fcfcfc;
      margin: 0;
      padding: 0;
    }

    a {
      color: #3273dc;
      text-decoration: none;
    }
    
    a:hover {
      color: #1e4b8f;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
    }

    .hero {
      text-align: center;
      padding: 40px 0;
    }

    .title {
      font-family: 'Google Sans', sans-serif;
      font-size: 2.5rem;
      font-weight: 700;
      line-height: 1.2;
      margin-bottom: 20px;
      color: #2c3e50;
    }

    .authors {
      font-size: 1.2rem;
      margin-bottom: 15px;
    }

    .author-block {
      display: inline-block;
      margin-right: 10px;
    }

    .affiliations {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 20px;
    }

    .publication-links {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
      margin-top: 20px;
    }

    .link-block {
      background-color: #363636;
      color: #fff;
      padding: 8px 16px;
      border-radius: 24px;
      font-weight: 600;
      font-size: 0.9rem;
      display: flex;
      align-items: center;
      transition: background-color 0.3s;
    }

    .link-block i {
      margin-right: 6px;
    }

    .link-block:hover {
      background-color: #4a4a4a;
      color: #fff;
    }

    .link-block.disabled {
      background-color: #b5b5b5;
      cursor: default;
    }

    .section {
      padding: 30px 0;
      border-bottom: 1px solid #eee;
    }

    .section:last-child {
      border-bottom: none;
    }

    .section-title {
      font-family: 'Google Sans', sans-serif;
      font-size: 1.8rem;
      font-weight: 600;
      text-align: center;
      margin-bottom: 30px;
      color: #2c3e50;
    }

    .abstract-text {
      text-align: justify;
      max-width: 800px;
      margin: 0 auto;
    }

    .image-container {
      text-align: center;
      margin: 30px 0;
    }

    .image-container img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    
    .caption {
      font-size: 0.9rem;
      color: #666;
      margin-top: 10px;
      font-style: italic;
    }

    /* Table Styles */
    .table-container {
      overflow-x: auto;
      margin: 20px auto;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
      margin: 0 auto;
    }

    th, td {
      padding: 10px;
      text-align: center;
      border-bottom: 1px solid #ddd;
    }

    th {
      background-color: #f5f5f5;
      font-weight: 600;
    }

    tr:hover {
      background-color: #f9f9f9;
    }

    .best {
      font-weight: bold;
      color: #d32f2f;
    }
    
    .second-best {
      font-weight: bold;
      text-decoration: underline;
    }

    /* Code block */
    pre {
      background-color: #f5f5f5;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
      font-size: 0.85rem;
      text-align: left;
    }

    .method-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin-top: 20px;
    }
    
    @media (max-width: 768px) {
      .method-grid {
        grid-template-columns: 1fr;
      }
    }

    .feature-box {
      background: #fff;
      border: 1px solid #eee;
      padding: 20px;
      border-radius: 8px;
    }

    .feature-title {
      font-weight: bold;
      margin-bottom: 10px;
      color: #209cee;
    }
    
    /* Placeholder for missing images */
    .img-placeholder {
      width: 100%;
      height: 300px;
      background-color: #e0e0e0;
      display: flex;
      align-items: center;
      justify-content: center;
      border-radius: 8px;
      color: #777;
      font-weight: bold;
      border: 2px dashed #bbb;
    }

  </style>
</head>
<body>

<div class="container">
  
  <!-- Header Section -->
  <div class="hero">
    <h1 class="title">UI-Mem: Self-Evolving Experience Memory for<br>Online Reinforcement Learning in Mobile GUI Agents</h1>
    
    <div class="authors">
      <span class="author-block">Han Xiao<sup>1*</sup>,</span>
      <span class="author-block">Guozhi Wang<sup>2*</sup>,</span>
      <span class="author-block">Hao Wang<sup>2*</sup>,</span>
      <span class="author-block">Shilong Liu<sup>3</sup>,</span>
      <span class="author-block">Yuxiang Chai<sup>1</sup>,</span>
      <span class="author-block">Yue Pan<sup>2</sup>,</span>
      <span class="author-block">Yufeng Zhou<sup>2</sup>,</span><br>
      <span class="author-block">Xiaoxin Chen<sup>2</sup>,</span>
      <span class="author-block">Yafei Wen<sup>2</sup>,</span>
      <span class="author-block">Hongsheng Li<sup>1‚Ä†</sup></span>
    </div>

    <div class="affiliations">
      <span class="author-block"><sup>1</sup>CUHK MMLab</span>
      <span class="author-block"><sup>2</sup>vivo AI Lab</span>
      <span class="author-block"><sup>3</sup>Princeton University</span><br>
      <span class="author-block"><sup>*</sup>Equal Contribution</span>
      <span class="author-block"><sup>‚Ä†</sup>Corresponding Author</span>
    </div>

    <div class="publication-links">
      <a href="#" class="link-block">
        <i class="fas fa-file-pdf"></i> Paper
      </a>
      <a href="#" class="link-block">
        <i class="fab fa-arxiv"></i> arXiv
      </a>
      <a href="#" class="link-block disabled">
        <i class="fab fa-github"></i> Code (Coming Soon)
      </a>
      <a href="#" class="link-block disabled">
        <i class="fas fa-database"></i> Data
      </a>
    </div>
  </div>

  <!-- Teaser Section -->
  <div class="section">
    <div class="image-container">
      <!-- REPLACE WITH YOUR FIGURE 1 IMAGE -->
      <img src="./static/images/teaser.png" alt="Comparison of RL paradigms" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex'">
      <div class="img-placeholder" style="display:none;">Figure 1: Comparison of RL paradigms (Place teaser.png in static/images)</div>
      <p class="caption">
        <strong>Figure 1. Comparison of RL paradigms for GUI agents.</strong> (a) Standard Online RL suffers from sparse rewards. (b) Experience Replay and (c) Dense Reward address sample efficiency but lack transfer. <strong>(d) UI-Mem (Ours)</strong> introduces an Evolving Memory that provides hierarchical guidance and continuously updates itself.
      </p>
    </div>
  </div>

  <!-- Abstract Section -->
  <div class="section">
    <h2 class="section-title">Abstract</h2>
    <div class="abstract-text">
      <p>
        Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose <strong>UI-Mem</strong>, a novel framework that enhances GUI online RL with a <strong>Hierarchical Experience Memory</strong>. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer.
      </p>
      <p>
        To effectively integrate memory guidance into online RL, we introduce <strong>Stratified Group Sampling</strong>, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a <strong>Self-Evolving Loop</strong> continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications.
      </p>
    </div>
  </div>

  <!-- Method Section -->
  <div class="section">
    <h2 class="section-title">Methodology</h2>
    
    <div class="image-container">
      <!-- REPLACE WITH YOUR FIGURE 2 IMAGE -->
      <img src="./static/images/framework.png" alt="UI-Mem Framework Overview" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex'">
      <div class="img-placeholder" style="display:none;">Figure 2: Framework Overview (Place framework.png in static/images)</div>
      <p class="caption">
        <strong>Figure 2. Overview of the UI-Mem framework.</strong> The agent retrieves hierarchical experience (Workflows, Subtask Skills, Failure Patterns). Stratified Group Sampling generates trajectories under varying guidance levels for Policy Optimization. The Self-Evolving Loop extracts abstract plans to update memory.
      </p>
    </div>

    <div class="method-grid">
      <div class="feature-box">
        <div class="feature-title"><i class="fas fa-layer-group"></i> Hierarchical Experience Memory</div>
        <p>
          We construct a structured memory pool that stores reusable knowledge at three levels:
          <ul>
            <li><strong>Workflows ($\mathcal{W}$):</strong> High-level plans capturing global strategies.</li>
            <li><strong>Subtask Skills ($\Sigma$):</strong> Atomic capabilities like "search for item" or "fill form".</li>
            <li><strong>Failure Patterns ($\mathcal{F}$):</strong> Common failure modes to prevent repetitive errors.</li>
          </ul>
          Experiences are stored as <em>parameterized templates</em> (e.g., "Send email to {{recipient}}") to enable cross-task transfer.
        </p>
      </div>

      <div class="feature-box">
        <div class="feature-title"><i class="fas fa-random"></i> Stratified Group Sampling</div>
        <p>
          To prevent the agent from becoming dependent on guidance, we inject varying levels of memory guidance into the GRPO rollout group:
          <ul>
            <li><strong>Strong Guidance:</strong> Full hierarchical plans to stabilize training.</li>
            <li><strong>Weak Guidance:</strong> Only high-level workflows to bridge planning and acting.</li>
            <li><strong>No Guidance:</strong> Pure exploration for unbiased policy estimation.</li>
          </ul>
        </p>
      </div>
    </div>

    <div class="feature-box" style="margin-top: 20px;">
      <div class="feature-title"><i class="fas fa-sync-alt"></i> Self-Evolving Loop</div>
      <p>
        A continuous loop that refines memory during training. Successful trajectories are abstracted into new plans (replacing concrete values with placeholders), while failed trajectories generate failure diagnoses. This ensures the memory co-evolves with the policy.
      </p>
    </div>
  </div>

  <!-- Results Section -->
  <div class="section">
    <h2 class="section-title">Experimental Results</h2>
    
    <p style="text-align: center; margin-bottom: 20px;">
      We evaluate UI-Mem on <strong>AndroidWorld</strong> and <strong>AndroidLab</strong> benchmarks.
    </p>

    <h3>üèÜ AndroidWorld Leaderboard</h3>
    <div class="table-container">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Params</th>
            <th>Success Rate (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: left;">Seed1.8 (Seed, 2025a)</td>
            <td>-</td>
            <td>70.7</td>
          </tr>
          <tr>
            <td style="text-align: left;">Gemini-2.5-Pro</td>
            <td>-</td>
            <td>69.7</td>
          </tr>
          <tr>
            <td style="text-align: left;">UI-Tars-1.5</td>
            <td>-</td>
            <td>64.2</td>
          </tr>
          <tr style="border-top: 2px solid #ccc;">
            <td style="text-align: left;">Qwen3-VL-8B (Base)</td>
            <td>8B</td>
            <td>47.6</td>
          </tr>
          <tr>
            <td style="text-align: left;"><strong>UI-Mem-8B (Ours)</strong></td>
            <td>8B</td>
            <td class="second-best">66.8</td>
          </tr>
          <tr style="background-color: #e8f5e9;">
            <td style="text-align: left;"><strong>UI-Mem-8B* (Ours + Retrieval)</strong></td>
            <td>8B</td>
            <td class="best">71.1</td>
          </tr>
          <tr style="border-top: 2px solid #ccc;">
            <td style="text-align: left;">Qwen3-VL-4B (Base)</td>
            <td>4B</td>
            <td>45.3</td>
          </tr>
          <tr>
            <td style="text-align: left;"><strong>UI-Mem-4B (Ours)</strong></td>
            <td>4B</td>
            <td>58.2</td>
          </tr>
          <tr style="background-color: #e8f5e9;">
            <td style="text-align: left;"><strong>UI-Mem-4B* (Ours + Retrieval)</strong></td>
            <td>4B</td>
            <td>62.5</td>
          </tr>
        </tbody>
      </table>
    </div>
    <p class="caption">
      * denotes inference-time memory retrieval. UI-Mem-8B outperforms closed-source commercial APIs like Gemini-2.5-Pro and Seed1.8.
    </p>

    <br>

    <h3>üìä AndroidLab Performance</h3>
    <div class="table-container">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Sub-Goal SR</th>
            <th>Reasonable Op Ratio</th>
            <th>Success Rate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: left;">GPT-4o</td>
            <td>35.0</td>
            <td>85.4</td>
            <td>31.2</td>
          </tr>
          <tr>
            <td style="text-align: left;">AutoGLM</td>
            <td>-</td>
            <td>-</td>
            <td>36.2</td>
          </tr>
          <tr>
            <td style="text-align: left;">MobileRL (7B)</td>
            <td>-</td>
            <td>-</td>
            <td>42.5</td>
          </tr>
          <tr style="background-color: #e8f5e9;">
            <td style="text-align: left;"><strong>UI-Mem-8B* (Ours)</strong></td>
            <td class="best">56.0</td>
            <td class="best">94.9</td>
            <td class="best">44.9</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>

  <!-- Qualitative Section -->
  <div class="section">
    <h2 class="section-title">Qualitative Analysis</h2>
    <div class="image-container">
      <!-- REPLACE WITH YOUR FIGURE 14 IMAGE -->
      <img src="./static/images/qualitative.png" alt="Qualitative Examples" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex'">
      <div class="img-placeholder" style="display:none;">Figure 14: Qualitative Examples (Place qualitative.png in static/images)</div>
      <p class="caption">
        <strong>Impact of Memory Guidance.</strong> Top: Full memory guidance leads to perfect execution. Middle: Weak guidance results in partial completion. Bottom: No guidance leads to failure. This validates that our stratified sampling provides diverse data for effective RL.
      </p>
    </div>
  </div>

  <!-- BibTeX Section -->
  <div class="section">
    <h2 class="section-title">BibTeX</h2>
    <pre><code>@article{xiao2026uimem,
  title={UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents},
  author={Xiao, Han and Wang, Guozhi and Wang, Hao and Liu, Shilong and Chai, Yuxiang and Pan, Yue and Zhou, Yufeng and Chen, Xiaoxin and Wen, Yafei and Li, Hongsheng},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
  </div>

  <footer style="text-align: center; padding: 20px; color: #666; font-size: 0.8rem;">
    <p>
      This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
    <p>
      Template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
    </p>
  </footer>

</div>

</body>
</html>